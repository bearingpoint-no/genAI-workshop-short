{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short GenAI Workshop\n",
    "\n",
    "In this workshop we will assist the toy manufacturerer \"LykkeLand LeketÃ¸ysfabrikk\". Their internal data warehouse is a mess and they want us to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Download the package created for the hackathon *'be-genai-workshop-short'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "    %pip install be-genai-workshop-short --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .env variables\n",
    "\n",
    "We use our own helper function to support running this both within google colab and regular Jupyter notebooks within the repository structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hackathon.env import load_env\n",
    "load_env(\n",
    "    load_type=\"interactive\",\n",
    "    use_defaults=True,\n",
    "    override=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "from openai import AzureOpenAI\n",
    "from IPython.display import Markdown\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    # api_version=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    # api_key=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    # api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_prompt(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Function that creates a post against openAI chatGPT service\n",
    "    in Azure AI from a string prompt and returns the first and\n",
    "    most deterministic response/completion.\n",
    "\n",
    "    :param prompt: A string prompt to be sent to the chatGPT service\n",
    "\n",
    "    :return: A string representation of the first answer proposed by the algorithm\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correct setup\n",
    "get_completion_from_prompt(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks: Data Cleaning of Product Reviews from Text Data\n",
    "\n",
    "LykkeLand has gathered a substantial amount of product reviews to assess customer satisfaction. Unfortunately, the data collection process overlooked the inclusion of critical numerical and categorical fields, which is a limiting factor for the quality of analysis available. The dataset comprises usernames, dates, locations, review titles, and the main body of the text, collected from customers in both Norway and abroad. However, it lacks detailed information about the specific products being reviewed. Despite these limitations, there's still potential for meaningful analysis by leveraging language models to impute missing details and categorize the reviews based on the available text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data\n",
    "The product reviews collected from customers over the past few months are stored in an Excel sheet. Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hackathon.data.fetch import product_reviews as fetch_product_reviews\n",
    "df = fetch_product_reviews()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a single review in the sheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_review = df.iloc[-1]\n",
    "\n",
    "pprint(single_review.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating and summarizing reviews\n",
    "To get a clear picture of what our customers think, we need to translate all reviews into a language we can read. It would also be useful with a dense summary. This way, we can quickly catch their general vibe and pinpoint the exact issues or compliments they're highlighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt which translates the review body into English and summarizes it\n",
    "\n",
    "def condense_and_translate_review(review: pd.Series) -> str:\n",
    "    prompt = f\"\"\"Please summarize the following review in one English sentence:\n",
    "Title: <title> {review[\"title\"]} </title>\n",
    "Review: <review> {review[\"body\"]} </review>    \n",
    "\"\"\"\n",
    "    return get_completion_from_prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: check if the function works as expected. The output should be a string\n",
    "# containing a summary of the review body in English\n",
    "\n",
    "pprint(condense_and_translate_review(single_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize and translate review body for each row in the dataframe. We use progress_apply introduced by tqdm for a progress bar instead of built-in Pandas apply.\n",
    "df[\"standardized_body\"] = df.progress_apply(lambda row: condense_and_translate_review(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"standardized_body\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find trends in the reviews\n",
    "We can also summarize all the rows to gain an overview of common complaints among customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review_bodies = \"\\n\\n\".join(df[\"standardized_body\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Summarize the following reviews:\"\"\" # your prompt here\n",
    "for review in df[\"standardized_body\"]:\n",
    "    prompt += f\"\\n\\n<review>{review}</review>\"\n",
    "prompt += \"\"\"Use the following output format:\n",
    "### Product related feedback\n",
    "**<product>**: <feedback summary> \\n\n",
    "\n",
    "### Other feedback\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Recommendations\n",
    "<recommendations for what LykkeLand can do to improve the products or customer satisfaction>\n",
    "\"\"\"\n",
    "# pprint(prompt)\n",
    "condensed_reviews = get_completion_from_prompt(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(condensed_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring country from location\n",
    "\n",
    "The location data column is less than ideal. Unfortunately, there has been no validation in the form when the customer filled out location. As you see below, sometimes location is written as \"city, country\", sometimes just \"city\", sometimes just country and sometimes even \"city, state\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(df[\"location\"].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a language model to standardize this information. Let's try to extract the country for each review.\n",
    "\n",
    "**Note:** A potential problem can be that there are multiple ways to write the same country name, i.e. Norway/Kingdom of Norway/Norge/Noreg etc. If you encounter this issue, how can it be fixed?\n",
    "\n",
    "Try to minimize the amount of information you give the model. Does it need all of the review information to do this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 1: Write prompt for inferring country from review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a prompt which classifies the country of the review based on the available unique countries\n",
    "\n",
    "def get_country(review: pd.Series) -> str:\n",
    "    prompt = f\"\"\"Return the country which this review is from. Just the English country name, nothing else.\n",
    "    \n",
    "    Location: {review.location}\n",
    "    Review title: {review.title}\n",
    "\"\"\" # your prompt here\n",
    "    country = get_completion_from_prompt(prompt)\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: check if the function works as expected. The output should be a string\n",
    "# containing the country of the review\n",
    "get_country(single_review) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new column called country where we perform this operation for every row in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the country for each review in the dataframe. We use progress_apply introduced by tqdm for a progress bar instead of built-in Pandas apply.\n",
    "df[\"country\"] = df.progress_apply(lambda row: get_country(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result of the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each country\n",
    "shipment_counts = df[\"country\"].value_counts().sort_index()\n",
    "\n",
    "# Create the bar plot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(\n",
    "    x=shipment_counts.index,\n",
    "    y=shipment_counts.values,\n",
    "    hue=shipment_counts.index,\n",
    "    palette=\"Blues_d\",\n",
    "    dodge=False,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%d', label_type='edge', padding=3)\n",
    "\n",
    "# Adjust y-axis limits to add space above the tallest bar\n",
    "ax.set_ylim(0, shipment_counts.max() * 1.1)  # Increase limit by 10%\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel(\"Country\", fontsize=14)\n",
    "ax.set_ylabel(\"Number of Shipments\", fontsize=14)\n",
    "ax.set_title(\"Number of Shipments to Each Country\", fontsize=16)\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify if you succeeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hackathon.evaluation.country_inferring import verify as verify_country_inferring\n",
    "\n",
    "verify_country_inferring(df, print_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating classification\n",
    "We aim to quantify a customer's satisfaction level by assigning a numerical score to the review text, for example, ranging from 1 to 5. This process will allow us to systematically evaluate and compare customer feedback, providing a clear metric to gauge overall contentment or dissatisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 2: Write prompt for rating each review between 1 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a prompt which classifies the rating of each review on a scale from 1 to 5\n",
    "\n",
    "def get_rating(review: pd.Series) -> int:\n",
    "    prompt = f\"\"\"Rate the following review on a scale from 1 to 5, where 1 is the lowest and 5 is the highest. Output only the number.\n",
    "\n",
    "<review>{review[\"body\"]}</review>\n",
    "\n",
    "Guidance:\n",
    "1: Very negative review\n",
    "2: Negative review\n",
    "3: Neutral review. Customer either both liked and disliked the product or was indifferent.\n",
    "4: Positive review\n",
    "5: Very positive review\n",
    "\"\"\"\n",
    "    rating = get_completion_from_prompt(prompt)\n",
    "    return int(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: check if the function works as expected. The output should be an integer\n",
    "# containing the rating of the review\n",
    "get_rating(single_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the rating for each review in the dataframe. We use progress_apply introduced by tqdm for a progress bar instead of built-in Pandas apply.\n",
    "df[\"rating\"] = df.progress_apply(lambda row: get_rating(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the rating distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each rating\n",
    "rating_counts = df[\"rating\"].value_counts().sort_index()\n",
    "\n",
    "# Create the bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(\n",
    "    x=rating_counts.index,\n",
    "    y=rating_counts.values,\n",
    "    hue=rating_counts.index,\n",
    "    palette=\"Blues_d\",\n",
    "    dodge=False,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel(\"Rating\", fontsize=14)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=14)\n",
    "ax.set_title(\"Frequency of Each Rating Value\", fontsize=16)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the average rating by country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the average rating by country\n",
    "average_ratings = df.groupby(\"country\")[\"rating\"].mean().sort_index()\n",
    "\n",
    "# Create the bar plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(\n",
    "    x=average_ratings.index,\n",
    "    y=average_ratings.values,\n",
    "    hue=average_ratings.index,\n",
    "    palette=\"Blues_d\",\n",
    "    edgecolor=\"darkblue\",\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel(\"Country\", fontsize=14)\n",
    "ax.set_ylabel(\"Average Rating\", fontsize=14)\n",
    "ax.set_title(\"Average Rating by Country\", fontsize=16)\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph you just plotted. What do you observe? We want you to extract **two** specific insights.\n",
    "\n",
    "**Note:** Simplify it. Don't consider that there can be errors due to small sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hackathon.evaluation.country_ratings_analysis import evaluate as evaluate_country_ratings_analysis\n",
    "\n",
    "YOUR_ANSWER = \"China has the most satisfied customers, USA has the least.\"\n",
    "\n",
    "print(evaluate_country_ratings_analysis(YOUR_ANSWER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize temporal trends\n",
    "We can create visual representations to track how ratings from a specific country evolve over time. By plotting these ratings, we can identify patterns, trends, or anomalies, offering valuable insights into how customer satisfaction may vary with different factors or events. This analysis could help in making informed decisions or adjustments in strategy based on temporal shifts in customer feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "france_ratings = df[df[\"country\"] == \"France\"].sort_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the line plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.lineplot(\n",
    "    x=\"date\",\n",
    "    y=\"rating\",\n",
    "    data=france_ratings,\n",
    "    marker=\"o\",\n",
    "    color=\"skyblue\",\n",
    "    errorbar=None\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel(\"Date\", fontsize=14)\n",
    "ax.set_ylabel(\"Rating\", fontsize=14)\n",
    "ax.set_title(\"Ratings Over Time for France\", fontsize=16)\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 3: Investigate this sudden detoriation in ratings for French customers\n",
    "\n",
    "Why does it seem like the ratings for France has detoriated after 2023-03-15? Find the answer by analyzing the (relevant) reviews using a LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
